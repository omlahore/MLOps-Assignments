{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9838ad2",
   "metadata": {},
   "source": [
    "## Step 1: Elasticsearch Client Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568273d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bonsai credentials and URL from environment variables\n",
    "BONSAI_HOST = os.getenv('BONSAI_HOST')\n",
    "ACCESS_KEY = os.getenv('ACCESS_KEY')\n",
    "ACCESS_SECRET = os.getenv('ACCESS_SECRET')\n",
    "\n",
    "# Set up Elasticsearch client\n",
    "es = Elasticsearch(\n",
    "    [{'host': BONSAI_HOST, 'port': 443, 'use_ssl': True}],\n",
    "    http_auth=(ACCESS_KEY, ACCESS_SECRET)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409488c1",
   "metadata": {},
   "source": [
    "Elasticsearch is a distributed search and analytics engine often used for log aggregation, full-text search, and more. \n",
    "\n",
    "In this example, we're connecting to Bonsai, a managed Elasticsearch service. However this can be any elastic search setup even from your local docker environment.\n",
    "\n",
    "Environment variables (BONSAI_HOST, ACCESS_KEY, ACCESS_SECRET) are used to securely manage sensitive data.\n",
    "The Elasticsearch client is initialized, which allows interaction with Elasticsearch for sending logs from our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae39990",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "# Create index for logs if it doesn't exist\n",
    "index_name = 'logs'\n",
    "try:\n",
    "    es.indices.create(index=index_name)\n",
    "except NotFoundError:\n",
    "    pass  # Index already exists\n",
    "except Exception as e:\n",
    "    print(\"Error creating index:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5522191a",
   "metadata": {},
   "source": [
    "#### Before logging data to Elasticsearch, we ensure that the logs index exists. Elasticsearch organizes data into indices, which are equivalent to tables in a relational database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticSearchHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        log_entry = self.format(record)\n",
    "        # Prepare the log entry for Elasticsearch\n",
    "        doc = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'level': record.levelname,\n",
    "            'message': log_entry,\n",
    "            'service': 'my_flask_app'\n",
    "        }\n",
    "        # Index the log entry in Elasticsearch\n",
    "        es.index(index=index_name, body=doc)\n",
    "\n",
    "# Set up the Elasticsearch logging handler\n",
    "handler = ElasticSearchHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(name)s :: %(levelname)-8s :: %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "app.logger.addHandler(handler)\n",
    "app.logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d8d44",
   "metadata": {},
   "source": [
    "We define a custom logging handler (ElasticSearchHandler) that sends log messages to Elasticsearch. The emit method is triggered each time a log entry is created.\n",
    "The log entry is formatted and indexed into Elasticsearch under the logs index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on dummy data\n",
    "X_train = np.array([[0, 0], [1, 1]])\n",
    "y_train = np.array([0, 1])\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fa907",
   "metadata": {},
   "source": [
    "A simple Logistic Regression model is created using scikit-learn.\n",
    "It is trained on dummy data with two features corresponding to binary outputs. In a real-world application, you would train the model on larger datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6afb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['GET'])\n",
    "def predict():\n",
    "    # Get query parameters\n",
    "    feature_1 = float(request.args.get('feature_1', 0))  # Default value is 0 if not provided\n",
    "    feature_2 = float(request.args.get('feature_2', 0))  # Default value is 0 if not provided\n",
    "\n",
    "    # Prepare input for the model\n",
    "    input_features = np.array([[feature_1, feature_2]])\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_features)\n",
    "\n",
    "    # Log the prediction\n",
    "    app.logger.info(f\"Prediction made: {prediction[0]} for features {input_features}\")\n",
    "\n",
    "    # Return prediction result as JSON\n",
    "    return jsonify({\n",
    "        'input': {\n",
    "            'feature_1': feature_1,\n",
    "            'feature_2': feature_2\n",
    "        },\n",
    "        'prediction': int(prediction[0])\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdfab31",
   "metadata": {},
   "source": [
    "The /predict route accepts two query parameters (feature_1 and feature_2), which are passed to the Logistic Regression model.\n",
    "The model makes a prediction based on the input features.\n",
    "The prediction result is logged to Elasticsearch and returned as JSON to the client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6f0fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Conclusion:\n",
    "1. Ephemeral Nature: Containerized applications (such as those running in Docker) are ephemeral, meaning they can be created and destroyed at any time. If you rely on local logs within the container, you risk losing important information when the container stops or restarts.\n",
    "2. Centralized and Preserved Logs: Remote logging solutions, such as Elasticsearch, allow you to centralize and preserve logs from various containers, ensuring that you have access to critical logs even after the container's lifecycle ends.\n",
    "3. Lifecycle of Instances: Elastic Compute Cloud (EC2) or similar cloud infrastructure instances may be auto-scaled, stopped, or replaced. Logging locally can result in log loss as instances are replaced or terminated.\n",
    "\n",
    "4. Persistent Storage of Logs: By sending logs to a remote logging solution like Elasticsearch, you ensure centralized access and persistent storage of logs, independent of the instanceâ€™s lifecycle.\n",
    "\n",
    "\n",
    "Benefits of Remote Logging\n",
    "1. Centralized Log Management: Remote logging solutions make it easier to aggregate logs from multiple containers, services, or instances. With Elasticsearch, you can easily monitor, search, and analyze logs from different sources in real-time using tools like Kibana.\n",
    "\n",
    "2. Monitoring and Troubleshooting: Remote logs provide an easier way to track issues, bugs, or performance bottlenecks by allowing you to inspect logs from multiple containers or services in a single location. Elasticsearch enables you to create queries and dashboards for better observability and troubleshooting.\n",
    "\n",
    "3. Compliance and Auditing: In some industries, regulations require logs to be stored for a certain period. Remote logging to Elasticsearch ensures logs are securely stored and can be retrieved if needed for audits or compliance checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bb54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
